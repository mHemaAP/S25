{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1bgmBUB6YuC-LuG-3X5QAXVEaWHaZEEsB","timestamp":1701974284367}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"WXu1r8qvSzWf"},"source":["# Twin-Delayed DDPG\n","\n","Complete credit goes to this [awesome Deep Reinforcement Learning 2.0 Course on Udemy](https://www.udemy.com/course/deep-reinforcement-learning/) for the code."]},{"cell_type":"markdown","metadata":{"id":"YRzQUhuUTc0J"},"source":["## Installing the packages"]},{"cell_type":"code","metadata":{"id":"HAHMB0Ze8fU0","outputId":"75156ded-363c-4360-ddd8-7675401adc4d","executionInfo":{"status":"ok","timestamp":1702531046398,"user_tz":-330,"elapsed":11929,"user":{"displayName":"Hema Medi","userId":"08506476599606964364"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install pybullet"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pybullet\n","  Downloading pybullet-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (103.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pybullet\n","Successfully installed pybullet-3.2.6\n"]}]},{"cell_type":"markdown","metadata":{"id":"Xjm2onHdT-Av"},"source":["## Importing the libraries"]},{"cell_type":"code","metadata":{"id":"Ikr2p0Js8iB4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702531050546,"user_tz":-330,"elapsed":4166,"user":{"displayName":"Hema Medi","userId":"08506476599606964364"}},"outputId":"bec44b60-75e1-41c8-cf46-25420ab056a2"},"source":["import os\n","import time\n","import random\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pybullet_envs\n","import gym\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from gym import wrappers\n","from torch.autograd import Variable\n","from collections import deque"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:440: UserWarning: \u001b[33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.\u001b[0m\n","  logger.warn(\n"]}]},{"cell_type":"markdown","metadata":{"id":"Y2nGdtlKVydr"},"source":["## Step 1: We initialize the Experience Replay memory"]},{"cell_type":"code","metadata":{"id":"u5rW0IDB8nTO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702531050547,"user_tz":-330,"elapsed":12,"user":{"displayName":"Hema Medi","userId":"08506476599606964364"}},"outputId":"20e18357-8cf1-4986-82e9-9b34625d34ff"},"source":["class ReplayBuffer(object):\n","\n","  def __init__(self, max_size=1e6):\n","    self.storage = []\n","    self.max_size = max_size\n","    self.ptr = 0\n","\n","  def add(self, transition):\n","    if len(self.storage) == self.max_size:\n","      self.storage[int(self.ptr)] = transition\n","      self.ptr = (self.ptr + 1) % self.max_size\n","    else:\n","      self.storage.append(transition)\n","      self.ptr = (self.ptr + 1) % self.max_size\n","\n","  def sample(self, batch_size):\n","    ind = np.random.randint(0, len(self.storage), size=batch_size)\n","    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n","    for i in ind:\n","      state, next_state, action, reward, done = self.storage[i]\n","      batch_states.append(np.array(state, copy=False))\n","      batch_next_states.append(np.array(next_state, copy=False))\n","      batch_actions.append(np.array(action, copy=False))\n","      batch_rewards.append(np.array(reward, copy=False))\n","      batch_dones.append(np.array(done, copy=False))\n","    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"markdown","metadata":{"id":"Jb7TTaHxWbQD"},"source":["## Step 2: We build one neural network for the Actor model and one neural network for the Actor target"]},{"cell_type":"code","metadata":{"id":"4CeRW4D79HL0"},"source":["class Actor(nn.Module):\n","\n","  def __init__(self, state_dim, action_dim, max_action):\n","    super(Actor, self).__init__()\n","    self.layer_1 = nn.Linear(state_dim, 400)\n","    self.layer_2 = nn.Linear(400, 300)\n","    self.layer_3 = nn.Linear(300, action_dim)\n","    self.max_action = max_action\n","\n","  def forward(self, x):\n","    x = F.relu(self.layer_1(x))\n","    x = F.relu(self.layer_2(x))\n","    x = self.max_action * torch.tanh(self.layer_3(x))\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HRDDce8FXef7"},"source":["## Step 3: We build two neural networks for the two Critic models and two neural networks for the two Critic targets"]},{"cell_type":"code","metadata":{"id":"OCee7gwR9Jrs"},"source":["class Critic(nn.Module):\n","\n","  def __init__(self, state_dim, action_dim):\n","    super(Critic, self).__init__()\n","    # Defining the first Critic neural network\n","    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n","    self.layer_2 = nn.Linear(400, 300)\n","    self.layer_3 = nn.Linear(300, 1)\n","    # Defining the second Critic neural network\n","    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n","    self.layer_5 = nn.Linear(400, 300)\n","    self.layer_6 = nn.Linear(300, 1)\n","\n","  def forward(self, x, u):\n","    xu = torch.cat([x, u], 1)\n","    # Forward-Propagation on the first Critic Neural Network\n","    x1 = F.relu(self.layer_1(xu))\n","    x1 = F.relu(self.layer_2(x1))\n","    x1 = self.layer_3(x1)\n","    # Forward-Propagation on the second Critic Neural Network\n","    x2 = F.relu(self.layer_4(xu))\n","    x2 = F.relu(self.layer_5(x2))\n","    x2 = self.layer_6(x2)\n","    return x1, x2\n","\n","  def Q1(self, x, u):\n","    xu = torch.cat([x, u], 1)\n","    x1 = F.relu(self.layer_1(xu))\n","    x1 = F.relu(self.layer_2(x1))\n","    x1 = self.layer_3(x1)\n","    return x1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NzIDuONodenW"},"source":["## Steps 4 to 15: Training Process"]},{"cell_type":"code","metadata":{"id":"zzd0H1xukdKe"},"source":["# Selecting the device (CPU or GPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Building the whole Training Process into a class\n","\n","class TD3(object):\n","\n","  def __init__(self, state_dim, action_dim, max_action):\n","    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n","    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n","    self.actor_target.load_state_dict(self.actor.state_dict())\n","    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n","    self.critic = Critic(state_dim, action_dim).to(device)\n","    self.critic_target = Critic(state_dim, action_dim).to(device)\n","    self.critic_target.load_state_dict(self.critic.state_dict())\n","    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n","    self.max_action = max_action\n","\n","  def select_action(self, state):\n","    state = torch.Tensor(state.reshape(1, -1)).to(device)\n","    return self.actor(state).cpu().data.numpy().flatten()\n","\n","  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n","\n","    for it in range(iterations):\n","\n","      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n","      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n","      state = torch.Tensor(batch_states).to(device)\n","      next_state = torch.Tensor(batch_next_states).to(device)\n","      action = torch.Tensor(batch_actions).to(device)\n","      reward = torch.Tensor(batch_rewards).to(device)\n","      done = torch.Tensor(batch_dones).to(device)\n","\n","      # Step 5: From the next state s’, the Actor target plays the next action a’\n","      next_action = self.actor_target(next_state)\n","\n","      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n","      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n","      noise = noise.clamp(-noise_clip, noise_clip)\n","      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n","\n","      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n","      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n","\n","      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n","      target_Q = torch.min(target_Q1, target_Q2)\n","\n","      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n","      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n","\n","      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n","      current_Q1, current_Q2 = self.critic(state, action)\n","\n","      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n","      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n","\n","      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n","      self.critic_optimizer.zero_grad()\n","      critic_loss.backward()\n","      self.critic_optimizer.step()\n","\n","      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n","      if it % policy_freq == 0:\n","        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n","        self.actor_optimizer.zero_grad()\n","        actor_loss.backward()\n","        self.actor_optimizer.step()\n","\n","        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n","        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n","          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n","\n","        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n","        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n","          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n","\n","  # Making a save method to save a trained model\n","  def save(self, filename, directory):\n","    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n","    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n","\n","  # Making a load method to load a pre-trained model\n","  def load(self, filename, directory):\n","    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n","    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ka-ZRtQvjBex"},"source":["## We make a function that evaluates the policy by calculating its average reward over 10 episodes"]},{"cell_type":"code","metadata":{"id":"qabqiYdp9wDM"},"source":["def evaluate_policy(policy, eval_episodes=10):\n","  avg_reward = 0.\n","  for _ in range(eval_episodes):\n","    obs = env.reset()\n","    done = False\n","    while not done:\n","      action = policy.select_action(np.array(obs))\n","      obs, reward, done, _ = env.step(action)\n","      avg_reward += reward\n","  avg_reward /= eval_episodes\n","  print (\"---------------------------------------\")\n","  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n","  print (\"---------------------------------------\")\n","  return avg_reward"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gGuKmH_ijf7U"},"source":["## We set the parameters"]},{"cell_type":"code","metadata":{"id":"HFj6wbAo97lk"},"source":["env_name = \"AntBulletEnv-v0\" # Name of a environment (set it to any Continous environment you want)\n","seed = 0 # Random seed number\n","start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n","eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n","max_timesteps = 5e5 # Total number of iterations/timesteps\n","save_models = True # Boolean checker whether or not to save the pre-trained model\n","expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n","batch_size = 100 # Size of the batch\n","discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n","tau = 0.005 # Target network update rate\n","policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n","noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n","policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hjwf2HCol3XP"},"source":["## We create a file name for the two saved models: the Actor and Critic models"]},{"cell_type":"code","metadata":{"id":"1fyH8N5z-o3o","outputId":"3a490058-afac-4626-c4cb-7cfc9a021bdb","executionInfo":{"status":"ok","timestamp":1702531051381,"user_tz":-330,"elapsed":13,"user":{"displayName":"Hema Medi","userId":"08506476599606964364"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n","print (\"---------------------------------------\")\n","print (\"Settings: %s\" % (file_name))\n","print (\"---------------------------------------\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["---------------------------------------\n","Settings: TD3_AntBulletEnv-v0_0\n","---------------------------------------\n"]}]},{"cell_type":"markdown","metadata":{"id":"kop-C96Aml8O"},"source":["## We create a folder inside which will be saved the trained models"]},{"cell_type":"code","metadata":{"id":"Src07lvY-zXb"},"source":["if not os.path.exists(\"./results\"):\n","  os.makedirs(\"./results\")\n","if save_models and not os.path.exists(\"./pytorch_models\"):\n","  os.makedirs(\"./pytorch_models\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qEAzOd47mv1Z"},"source":["## We create the PyBullet environment"]},{"cell_type":"code","metadata":{"id":"CyQXJUIs-6BV","outputId":"57c140dd-4165-43d0-e1ae-77afdb99c45b","executionInfo":{"status":"ok","timestamp":1702531051381,"user_tz":-330,"elapsed":9,"user":{"displayName":"Hema Medi","userId":"08506476599606964364"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["env = gym.make(env_name)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]}]},{"cell_type":"markdown","metadata":{"id":"5YdPG4HXnNsh"},"source":["## We set seeds and we get the necessary information on the states and actions in the chosen environment"]},{"cell_type":"code","metadata":{"id":"Z3RufYec_ADj"},"source":["env.seed(seed)\n","torch.manual_seed(seed)\n","np.random.seed(seed)\n","state_dim = env.observation_space.shape[0]\n","action_dim = env.action_space.shape[0]\n","max_action = float(env.action_space.high[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HWEgDAQxnbem"},"source":["## We create the policy network (the Actor model)"]},{"cell_type":"code","metadata":{"id":"wTVvG7F8_EWg"},"source":["policy = TD3(state_dim, action_dim, max_action)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZI60VN2Unklh"},"source":["[link text](https://)## We create the Experience Replay memory"]},{"cell_type":"code","metadata":{"id":"sd-ZsdXR_LgV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702531058948,"user_tz":-330,"elapsed":20,"user":{"displayName":"Hema Medi","userId":"08506476599606964364"}},"outputId":"381aadec-3232-4ad6-bf3d-bf294f0ee912"},"source":["replay_buffer = ReplayBuffer()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"markdown","metadata":{"id":"QYOpCyiDnw7s"},"source":["## We define a list where all the evaluation results over 10 episodes are stored"]},{"cell_type":"code","metadata":{"id":"dhC_5XJ__Orp","outputId":"e034773d-3929-4316-92c2-a23c3a045de4","executionInfo":{"status":"ok","timestamp":1702531062235,"user_tz":-330,"elapsed":3304,"user":{"displayName":"Hema Medi","userId":"08506476599606964364"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["evaluations = [evaluate_policy(policy)]"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n","  logger.deprecation(\n"]},{"output_type":"stream","name":"stdout","text":["---------------------------------------\n","Average Reward over the Evaluation Step: 9.807990\n","---------------------------------------\n"]}]},{"cell_type":"markdown","metadata":{"id":"xm-4b3p6rglE"},"source":["## We create a new folder directory in which the final results (videos of the agent) will be populated"]},{"cell_type":"code","metadata":{"id":"MTL9uMd0ru03"},"source":["def mkdir(base, name):\n","    path = os.path.join(base, name)\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","    return path\n","work_dir = mkdir('exp', 'brs')\n","monitor_dir = mkdir(work_dir, 'monitor')\n","max_episode_steps = env._max_episode_steps\n","save_env_vid = False\n","if save_env_vid:\n","  env = wrappers.Monitor(env, monitor_dir, force = True)\n","  env.reset()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"31n5eb03p-Fm"},"source":["## We initialize the variables"]},{"cell_type":"code","metadata":{"id":"1vN5EvxK_QhT"},"source":["total_timesteps = 0\n","timesteps_since_eval = 0\n","episode_num = 0\n","done = True\n","t0 = time.time()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q9gsjvtPqLgT"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"y_ouY4NH_Y0I","outputId":"9e79ea42-4509-491c-e41f-7b67284c4ae1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702535999718,"user_tz":-330,"elapsed":4937495,"user":{"displayName":"Hema Medi","userId":"08506476599606964364"}}},"source":["max_timesteps = 500000\n","# We start the main loop over 500,000 timesteps\n","while total_timesteps < max_timesteps:\n","\n","  # If the episode is done\n","  if done:\n","\n","    # If we are not at the very beginning, we start the training process of the model\n","    if total_timesteps != 0:\n","      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n","      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n","\n","    # We evaluate the episode and we save the policy\n","    if timesteps_since_eval >= eval_freq:\n","      timesteps_since_eval %= eval_freq\n","      evaluations.append(evaluate_policy(policy))\n","      policy.save(file_name, directory=\"./pytorch_models\")\n","      np.save(\"./results/%s\" % (file_name), evaluations)\n","\n","    # When the training step is done, we reset the state of the environment\n","    obs = env.reset()\n","\n","    # Set the Done to False\n","    done = False\n","\n","    # Set rewards and episode timesteps to zero\n","    episode_reward = 0\n","    episode_timesteps = 0\n","    episode_num += 1\n","\n","  # Before 10000 timesteps, we play random actions\n","  if total_timesteps < start_timesteps:\n","    action = env.action_space.sample()\n","  else: # After 10000 timesteps, we switch to the model\n","    action = policy.select_action(np.array(obs))\n","    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n","    if expl_noise != 0:\n","      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n","\n","  # The agent performs the action in the environment, then reaches the next state and receives the reward\n","  new_obs, reward, done, _ = env.step(action)\n","\n","  # We check if the episode is done\n","  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n","\n","  # We increase the total reward\n","  episode_reward += reward\n","\n","  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n","  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n","\n","  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n","  obs = new_obs\n","  episode_timesteps += 1\n","  total_timesteps += 1\n","  timesteps_since_eval += 1\n","\n","# We add the last policy evaluation to our list of evaluations and we save our model\n","evaluations.append(evaluate_policy(policy))\n","if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n","np.save(\"./results/%s\" % (file_name), evaluations)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total Timesteps: 526 Episode Num: 1 Reward: 250.64413691211945\n","Total Timesteps: 1526 Episode Num: 2 Reward: 509.24949997036754\n","Total Timesteps: 1725 Episode Num: 3 Reward: 94.38306663120605\n","Total Timesteps: 2725 Episode Num: 4 Reward: 508.9672473353051\n","Total Timesteps: 2751 Episode Num: 5 Reward: 8.61969949683262\n","Total Timesteps: 3751 Episode Num: 6 Reward: 500.63124899143287\n","Total Timesteps: 3814 Episode Num: 7 Reward: 24.089118781419533\n","Total Timesteps: 4814 Episode Num: 8 Reward: 499.1491590960802\n","Total Timesteps: 5814 Episode Num: 9 Reward: 479.69717030487635\n","---------------------------------------\n","Average Reward over the Evaluation Step: 67.790358\n","---------------------------------------\n","Total Timesteps: 6814 Episode Num: 10 Reward: 482.2224111056716\n","Total Timesteps: 7814 Episode Num: 11 Reward: 514.5894390849433\n","Total Timesteps: 7845 Episode Num: 12 Reward: 10.552361275846486\n","Total Timesteps: 8845 Episode Num: 13 Reward: 504.6164464855492\n","Total Timesteps: 9845 Episode Num: 14 Reward: 499.9171061711177\n","Total Timesteps: 10515 Episode Num: 15 Reward: 263.9074996987755\n","---------------------------------------\n","Average Reward over the Evaluation Step: 178.969004\n","---------------------------------------\n","Total Timesteps: 11515 Episode Num: 16 Reward: 180.28519235613697\n","Total Timesteps: 11562 Episode Num: 17 Reward: 2.8178867715300315\n","Total Timesteps: 11882 Episode Num: 18 Reward: 53.92147644059993\n","Total Timesteps: 11902 Episode Num: 19 Reward: -1.9541258364529281\n","Total Timesteps: 11922 Episode Num: 20 Reward: -1.844742045527846\n","Total Timesteps: 11942 Episode Num: 21 Reward: -1.4626972393980047\n","Total Timesteps: 11962 Episode Num: 22 Reward: -2.026501750417744\n","Total Timesteps: 11982 Episode Num: 23 Reward: -2.0617277762312995\n","Total Timesteps: 12002 Episode Num: 24 Reward: -1.7380597649365448\n","Total Timesteps: 13002 Episode Num: 25 Reward: 125.54460440625654\n","Total Timesteps: 14002 Episode Num: 26 Reward: 253.61892822265685\n","Total Timesteps: 15002 Episode Num: 27 Reward: 130.0621615494318\n","---------------------------------------\n","Average Reward over the Evaluation Step: 129.565029\n","---------------------------------------\n","Total Timesteps: 16002 Episode Num: 28 Reward: 239.06177886577598\n","Total Timesteps: 17002 Episode Num: 29 Reward: 295.1968157980213\n","Total Timesteps: 18002 Episode Num: 30 Reward: 203.29452819155418\n","Total Timesteps: 19002 Episode Num: 31 Reward: 232.65228497813393\n","Total Timesteps: 20002 Episode Num: 32 Reward: 214.12815782546227\n","---------------------------------------\n","Average Reward over the Evaluation Step: 396.419931\n","---------------------------------------\n","Total Timesteps: 21002 Episode Num: 33 Reward: 417.36414461090953\n","Total Timesteps: 21390 Episode Num: 34 Reward: 82.83927468456457\n","Total Timesteps: 22390 Episode Num: 35 Reward: 387.8049996442139\n","Total Timesteps: 22410 Episode Num: 36 Reward: 1.911326841605249\n","Total Timesteps: 22430 Episode Num: 37 Reward: 2.7247643277403197\n","Total Timesteps: 22450 Episode Num: 38 Reward: 2.4513745872232824\n","Total Timesteps: 22470 Episode Num: 39 Reward: 1.2938462657098406\n","Total Timesteps: 22490 Episode Num: 40 Reward: 1.593593768583868\n","Total Timesteps: 22510 Episode Num: 41 Reward: 2.2931895896154186\n","Total Timesteps: 23510 Episode Num: 42 Reward: 318.5178310557985\n","Total Timesteps: 24510 Episode Num: 43 Reward: 271.11778038657496\n","Total Timesteps: 25510 Episode Num: 44 Reward: 219.89791459805315\n","---------------------------------------\n","Average Reward over the Evaluation Step: 333.874923\n","---------------------------------------\n","Total Timesteps: 26510 Episode Num: 45 Reward: 236.47912612523393\n","Total Timesteps: 27510 Episode Num: 46 Reward: 382.59268686591787\n","Total Timesteps: 28510 Episode Num: 47 Reward: 549.4195405403182\n","Total Timesteps: 29510 Episode Num: 48 Reward: 415.69947739794907\n","Total Timesteps: 29681 Episode Num: 49 Reward: 41.8107901659054\n","Total Timesteps: 29707 Episode Num: 50 Reward: 5.843646793540634\n","Total Timesteps: 29735 Episode Num: 51 Reward: 6.017685772344114\n","Total Timesteps: 29763 Episode Num: 52 Reward: 7.256922887951708\n","Total Timesteps: 29803 Episode Num: 53 Reward: 10.899703188625255\n","Total Timesteps: 30803 Episode Num: 54 Reward: 534.3255910765703\n","---------------------------------------\n","Average Reward over the Evaluation Step: 524.869102\n","---------------------------------------\n","Total Timesteps: 31803 Episode Num: 55 Reward: 484.7940573537716\n","Total Timesteps: 32803 Episode Num: 56 Reward: 530.3732480183654\n","Total Timesteps: 33803 Episode Num: 57 Reward: 738.9059230439627\n","Total Timesteps: 34803 Episode Num: 58 Reward: 432.0630555729951\n","Total Timesteps: 35803 Episode Num: 59 Reward: 452.38582684855345\n","---------------------------------------\n","Average Reward over the Evaluation Step: 549.297481\n","---------------------------------------\n","Total Timesteps: 36803 Episode Num: 60 Reward: 653.6873337180676\n","Total Timesteps: 37803 Episode Num: 61 Reward: 227.84743830979454\n","Total Timesteps: 38803 Episode Num: 62 Reward: 225.0822506140628\n","Total Timesteps: 39803 Episode Num: 63 Reward: 667.5562829429908\n","Total Timesteps: 40803 Episode Num: 64 Reward: 189.9301462743149\n","---------------------------------------\n","Average Reward over the Evaluation Step: 250.524464\n","---------------------------------------\n","Total Timesteps: 41803 Episode Num: 65 Reward: 321.15230018504917\n","Total Timesteps: 42803 Episode Num: 66 Reward: 300.41461471580715\n","Total Timesteps: 43803 Episode Num: 67 Reward: 473.29299289432373\n","Total Timesteps: 44803 Episode Num: 68 Reward: 339.8962198543023\n","Total Timesteps: 45803 Episode Num: 69 Reward: 156.13006061175903\n","---------------------------------------\n","Average Reward over the Evaluation Step: 239.574497\n","---------------------------------------\n","Total Timesteps: 46803 Episode Num: 70 Reward: 255.92340707477592\n","Total Timesteps: 47803 Episode Num: 71 Reward: 306.0749305751452\n","Total Timesteps: 48803 Episode Num: 72 Reward: 135.95425161961614\n","Total Timesteps: 49803 Episode Num: 73 Reward: 488.2898319036591\n","Total Timesteps: 50803 Episode Num: 74 Reward: 697.9226803196232\n","---------------------------------------\n","Average Reward over the Evaluation Step: 235.706655\n","---------------------------------------\n","Total Timesteps: 51803 Episode Num: 75 Reward: 324.47379046390375\n","Total Timesteps: 52803 Episode Num: 76 Reward: 549.559277037346\n","Total Timesteps: 53803 Episode Num: 77 Reward: 185.97891931524444\n","Total Timesteps: 54803 Episode Num: 78 Reward: 729.6882842459383\n","Total Timesteps: 55803 Episode Num: 79 Reward: 450.29198639534485\n","---------------------------------------\n","Average Reward over the Evaluation Step: 272.660582\n","---------------------------------------\n","Total Timesteps: 56803 Episode Num: 80 Reward: 235.22797813688857\n","Total Timesteps: 57803 Episode Num: 81 Reward: 486.0147923674356\n","Total Timesteps: 58803 Episode Num: 82 Reward: 315.9674412873337\n","Total Timesteps: 59803 Episode Num: 83 Reward: 417.55878383153316\n","Total Timesteps: 60803 Episode Num: 84 Reward: 529.783385783332\n","---------------------------------------\n","Average Reward over the Evaluation Step: 329.681674\n","---------------------------------------\n","Total Timesteps: 61803 Episode Num: 85 Reward: 312.08507938424924\n","Total Timesteps: 62803 Episode Num: 86 Reward: 526.9044523644118\n","Total Timesteps: 63803 Episode Num: 87 Reward: 427.31656487497423\n","Total Timesteps: 64803 Episode Num: 88 Reward: 532.2727237371299\n","Total Timesteps: 65803 Episode Num: 89 Reward: 252.51570266271452\n","---------------------------------------\n","Average Reward over the Evaluation Step: 407.693211\n","---------------------------------------\n","Total Timesteps: 66803 Episode Num: 90 Reward: 400.1883652750585\n","Total Timesteps: 67803 Episode Num: 91 Reward: 554.9935784809941\n","Total Timesteps: 68803 Episode Num: 92 Reward: 456.08393774234133\n","Total Timesteps: 69803 Episode Num: 93 Reward: 238.2806066269189\n","Total Timesteps: 70803 Episode Num: 94 Reward: 405.7985714723235\n","---------------------------------------\n","Average Reward over the Evaluation Step: 56.643590\n","---------------------------------------\n","Total Timesteps: 70823 Episode Num: 95 Reward: 4.307834935695055\n","Total Timesteps: 70843 Episode Num: 96 Reward: 3.6683473184045283\n","Total Timesteps: 71843 Episode Num: 97 Reward: 211.31279153728698\n","Total Timesteps: 72843 Episode Num: 98 Reward: 517.7410395322701\n","Total Timesteps: 73843 Episode Num: 99 Reward: 371.23226214096525\n","Total Timesteps: 74843 Episode Num: 100 Reward: 532.0179381285354\n","Total Timesteps: 75843 Episode Num: 101 Reward: 288.8919150136349\n","---------------------------------------\n","Average Reward over the Evaluation Step: 327.873779\n","---------------------------------------\n","Total Timesteps: 76843 Episode Num: 102 Reward: 236.71388823778614\n","Total Timesteps: 77843 Episode Num: 103 Reward: 606.5603617633853\n","Total Timesteps: 78843 Episode Num: 104 Reward: 382.84371426929096\n","Total Timesteps: 79843 Episode Num: 105 Reward: 484.8239822559607\n","Total Timesteps: 80843 Episode Num: 106 Reward: 420.139392008076\n","---------------------------------------\n","Average Reward over the Evaluation Step: 457.114181\n","---------------------------------------\n","Total Timesteps: 81843 Episode Num: 107 Reward: 419.97078414551163\n","Total Timesteps: 82843 Episode Num: 108 Reward: 399.56871232350704\n","Total Timesteps: 83843 Episode Num: 109 Reward: 376.94175182834425\n","Total Timesteps: 84843 Episode Num: 110 Reward: 367.7006906416628\n","Total Timesteps: 85843 Episode Num: 111 Reward: 246.36760517298566\n","---------------------------------------\n","Average Reward over the Evaluation Step: 522.319085\n","---------------------------------------\n","Total Timesteps: 86843 Episode Num: 112 Reward: 674.2923893401359\n","Total Timesteps: 87843 Episode Num: 113 Reward: 590.8924551406972\n","Total Timesteps: 88843 Episode Num: 114 Reward: 327.7990844793306\n","Total Timesteps: 89843 Episode Num: 115 Reward: 380.9776064494601\n","Total Timesteps: 90843 Episode Num: 116 Reward: 432.4593572659742\n","---------------------------------------\n","Average Reward over the Evaluation Step: 512.462975\n","---------------------------------------\n","Total Timesteps: 91843 Episode Num: 117 Reward: 463.96018160131285\n","Total Timesteps: 92843 Episode Num: 118 Reward: 584.7700114753354\n","Total Timesteps: 93843 Episode Num: 119 Reward: 308.9796066101593\n","Total Timesteps: 94843 Episode Num: 120 Reward: 128.81634594282792\n","Total Timesteps: 95843 Episode Num: 121 Reward: 321.0742610908706\n","---------------------------------------\n","Average Reward over the Evaluation Step: 524.876204\n","---------------------------------------\n","Total Timesteps: 96843 Episode Num: 122 Reward: 344.7869985221083\n","Total Timesteps: 97843 Episode Num: 123 Reward: 497.166836093759\n","Total Timesteps: 98843 Episode Num: 124 Reward: 560.3553914498005\n","Total Timesteps: 99843 Episode Num: 125 Reward: 454.44201266152083\n","Total Timesteps: 100843 Episode Num: 126 Reward: 580.1250188142394\n","---------------------------------------\n","Average Reward over the Evaluation Step: 513.183342\n","---------------------------------------\n","Total Timesteps: 101843 Episode Num: 127 Reward: 499.9069443481002\n","Total Timesteps: 102843 Episode Num: 128 Reward: 536.2530654342643\n","Total Timesteps: 103843 Episode Num: 129 Reward: 482.8284035129403\n","Total Timesteps: 104843 Episode Num: 130 Reward: 406.55315054129005\n","Total Timesteps: 105843 Episode Num: 131 Reward: 550.0687921536161\n","---------------------------------------\n","Average Reward over the Evaluation Step: 723.029688\n","---------------------------------------\n","Total Timesteps: 106843 Episode Num: 132 Reward: 806.445051461451\n","Total Timesteps: 107843 Episode Num: 133 Reward: 786.8648271598978\n","Total Timesteps: 108843 Episode Num: 134 Reward: 495.13358538138385\n","Total Timesteps: 109843 Episode Num: 135 Reward: 405.8661512215626\n","Total Timesteps: 110843 Episode Num: 136 Reward: 354.84374583648736\n","---------------------------------------\n","Average Reward over the Evaluation Step: 414.342540\n","---------------------------------------\n","Total Timesteps: 111843 Episode Num: 137 Reward: 367.94186532485764\n","Total Timesteps: 112843 Episode Num: 138 Reward: 545.4617061460706\n","Total Timesteps: 113843 Episode Num: 139 Reward: 679.3846493543233\n","Total Timesteps: 114843 Episode Num: 140 Reward: 490.36896785324365\n","Total Timesteps: 115843 Episode Num: 141 Reward: 662.5807692947938\n","---------------------------------------\n","Average Reward over the Evaluation Step: 514.082624\n","---------------------------------------\n","Total Timesteps: 116843 Episode Num: 142 Reward: 289.83357140539556\n","Total Timesteps: 117843 Episode Num: 143 Reward: 343.38891269840025\n","Total Timesteps: 118843 Episode Num: 144 Reward: 345.2516743774123\n","Total Timesteps: 119843 Episode Num: 145 Reward: 480.5727469651964\n","Total Timesteps: 120843 Episode Num: 146 Reward: 389.47179755367193\n","---------------------------------------\n","Average Reward over the Evaluation Step: 479.799688\n","---------------------------------------\n","Total Timesteps: 121843 Episode Num: 147 Reward: 391.8771730475262\n","Total Timesteps: 122843 Episode Num: 148 Reward: 562.9421156068204\n","Total Timesteps: 123843 Episode Num: 149 Reward: 726.6697010836334\n","Total Timesteps: 124843 Episode Num: 150 Reward: 818.3968205594157\n","Total Timesteps: 125843 Episode Num: 151 Reward: 463.1030079083185\n","---------------------------------------\n","Average Reward over the Evaluation Step: 369.398496\n","---------------------------------------\n","Total Timesteps: 126843 Episode Num: 152 Reward: 420.31690616264535\n","Total Timesteps: 127843 Episode Num: 153 Reward: 416.5926850435593\n","Total Timesteps: 128843 Episode Num: 154 Reward: 674.2777851002762\n","Total Timesteps: 129843 Episode Num: 155 Reward: 508.0098570015652\n","Total Timesteps: 130843 Episode Num: 156 Reward: 691.3600816363315\n","---------------------------------------\n","Average Reward over the Evaluation Step: 509.314554\n","---------------------------------------\n","Total Timesteps: 131843 Episode Num: 157 Reward: 585.9696431168385\n","Total Timesteps: 132843 Episode Num: 158 Reward: 328.2713629767903\n","Total Timesteps: 133843 Episode Num: 159 Reward: 648.2195091534737\n","Total Timesteps: 134843 Episode Num: 160 Reward: 428.8430306535047\n","Total Timesteps: 135843 Episode Num: 161 Reward: 578.0072134760874\n","---------------------------------------\n","Average Reward over the Evaluation Step: 679.804203\n","---------------------------------------\n","Total Timesteps: 136843 Episode Num: 162 Reward: 587.426709074499\n","Total Timesteps: 137843 Episode Num: 163 Reward: 684.70790052417\n","Total Timesteps: 138843 Episode Num: 164 Reward: 733.1756551368696\n","Total Timesteps: 139843 Episode Num: 165 Reward: 410.0030670361327\n","Total Timesteps: 140843 Episode Num: 166 Reward: 547.8916911316234\n","---------------------------------------\n","Average Reward over the Evaluation Step: 582.617412\n","---------------------------------------\n","Total Timesteps: 141843 Episode Num: 167 Reward: 381.7654232931278\n","Total Timesteps: 142843 Episode Num: 168 Reward: 623.6063856280284\n","Total Timesteps: 143843 Episode Num: 169 Reward: 435.05619494199976\n","Total Timesteps: 144843 Episode Num: 170 Reward: 466.90173965749284\n","Total Timesteps: 145843 Episode Num: 171 Reward: 604.2320692706512\n","---------------------------------------\n","Average Reward over the Evaluation Step: 385.151481\n","---------------------------------------\n","Total Timesteps: 146843 Episode Num: 172 Reward: 350.38633403284405\n","Total Timesteps: 147843 Episode Num: 173 Reward: 636.0572065390483\n","Total Timesteps: 148843 Episode Num: 174 Reward: 598.0163725472909\n","Total Timesteps: 149843 Episode Num: 175 Reward: 595.8312870711761\n","Total Timesteps: 150843 Episode Num: 176 Reward: 462.7208537977473\n","---------------------------------------\n","Average Reward over the Evaluation Step: 485.264910\n","---------------------------------------\n","Total Timesteps: 151843 Episode Num: 177 Reward: 467.2780142891777\n","Total Timesteps: 152843 Episode Num: 178 Reward: 604.5890383247475\n","Total Timesteps: 153843 Episode Num: 179 Reward: 654.8034617811853\n","Total Timesteps: 154843 Episode Num: 180 Reward: 803.9953640959969\n","Total Timesteps: 155843 Episode Num: 181 Reward: 425.36182476576516\n","---------------------------------------\n","Average Reward over the Evaluation Step: 250.313034\n","---------------------------------------\n","Total Timesteps: 156843 Episode Num: 182 Reward: 231.87321880436153\n","Total Timesteps: 157843 Episode Num: 183 Reward: 370.4174548737292\n","Total Timesteps: 158843 Episode Num: 184 Reward: 436.53893677720947\n","Total Timesteps: 159843 Episode Num: 185 Reward: 521.7964711927563\n","Total Timesteps: 160843 Episode Num: 186 Reward: 749.8819160355588\n","---------------------------------------\n","Average Reward over the Evaluation Step: 692.028132\n","---------------------------------------\n","Total Timesteps: 161843 Episode Num: 187 Reward: 687.6208853024882\n","Total Timesteps: 162843 Episode Num: 188 Reward: 691.0518861367873\n","Total Timesteps: 163843 Episode Num: 189 Reward: 393.1208900497627\n","Total Timesteps: 164843 Episode Num: 190 Reward: 288.6824666932644\n","Total Timesteps: 165843 Episode Num: 191 Reward: 472.048983228664\n","---------------------------------------\n","Average Reward over the Evaluation Step: 469.670675\n","---------------------------------------\n","Total Timesteps: 166843 Episode Num: 192 Reward: 543.6674550810304\n","Total Timesteps: 167843 Episode Num: 193 Reward: 467.8470100187253\n","Total Timesteps: 168843 Episode Num: 194 Reward: 442.33015014386393\n","Total Timesteps: 169843 Episode Num: 195 Reward: 531.0927194033684\n","Total Timesteps: 170843 Episode Num: 196 Reward: 549.3808961065031\n","---------------------------------------\n","Average Reward over the Evaluation Step: 586.812880\n","---------------------------------------\n","Total Timesteps: 171843 Episode Num: 197 Reward: 592.3966173025296\n","Total Timesteps: 172843 Episode Num: 198 Reward: 396.32805979683445\n","Total Timesteps: 173843 Episode Num: 199 Reward: 759.5136172183218\n","Total Timesteps: 174843 Episode Num: 200 Reward: 574.35849495977\n","Total Timesteps: 175843 Episode Num: 201 Reward: 442.46308476478737\n","---------------------------------------\n","Average Reward over the Evaluation Step: 614.266511\n","---------------------------------------\n","Total Timesteps: 176843 Episode Num: 202 Reward: 539.9317505198657\n","Total Timesteps: 177843 Episode Num: 203 Reward: 589.45566813458\n","Total Timesteps: 178843 Episode Num: 204 Reward: 775.4252453434532\n","Total Timesteps: 179843 Episode Num: 205 Reward: 663.9975026942739\n","Total Timesteps: 180843 Episode Num: 206 Reward: 816.5039774544546\n","---------------------------------------\n","Average Reward over the Evaluation Step: 650.727067\n","---------------------------------------\n","Total Timesteps: 181843 Episode Num: 207 Reward: 632.7495995485363\n","Total Timesteps: 182843 Episode Num: 208 Reward: 772.5044426568843\n","Total Timesteps: 183843 Episode Num: 209 Reward: 597.5634468603546\n","Total Timesteps: 184843 Episode Num: 210 Reward: 614.4689955865505\n","Total Timesteps: 185843 Episode Num: 211 Reward: 521.8942172564058\n","---------------------------------------\n","Average Reward over the Evaluation Step: 597.877976\n","---------------------------------------\n","Total Timesteps: 186843 Episode Num: 212 Reward: 406.6319578630009\n","Total Timesteps: 187843 Episode Num: 213 Reward: 396.19426389819483\n","Total Timesteps: 188843 Episode Num: 214 Reward: 676.9605147624598\n","Total Timesteps: 189843 Episode Num: 215 Reward: 662.6067637472052\n","Total Timesteps: 190843 Episode Num: 216 Reward: 643.539647747289\n","---------------------------------------\n","Average Reward over the Evaluation Step: 682.749437\n","---------------------------------------\n","Total Timesteps: 191843 Episode Num: 217 Reward: 611.2040021756125\n","Total Timesteps: 192843 Episode Num: 218 Reward: 721.5495719082281\n","Total Timesteps: 193843 Episode Num: 219 Reward: 526.0599396861388\n","Total Timesteps: 194843 Episode Num: 220 Reward: 471.5534526934342\n","Total Timesteps: 195843 Episode Num: 221 Reward: 673.3042364329899\n","---------------------------------------\n","Average Reward over the Evaluation Step: 677.292888\n","---------------------------------------\n","Total Timesteps: 196843 Episode Num: 222 Reward: 793.9019681068374\n","Total Timesteps: 197843 Episode Num: 223 Reward: 452.7547216684741\n","Total Timesteps: 198843 Episode Num: 224 Reward: 592.9125343676322\n","Total Timesteps: 199843 Episode Num: 225 Reward: 594.3414302908202\n","Total Timesteps: 200843 Episode Num: 226 Reward: 511.96037615852225\n","---------------------------------------\n","Average Reward over the Evaluation Step: 621.252403\n","---------------------------------------\n","Total Timesteps: 201843 Episode Num: 227 Reward: 603.3127892350327\n","Total Timesteps: 202843 Episode Num: 228 Reward: 598.7603956875848\n","Total Timesteps: 203843 Episode Num: 229 Reward: 731.6592433482408\n","Total Timesteps: 204843 Episode Num: 230 Reward: 650.3360799075504\n","Total Timesteps: 205843 Episode Num: 231 Reward: 703.6874512098337\n","---------------------------------------\n","Average Reward over the Evaluation Step: 649.020217\n","---------------------------------------\n","Total Timesteps: 206843 Episode Num: 232 Reward: 603.4088449468784\n","Total Timesteps: 207843 Episode Num: 233 Reward: 879.1055964104146\n","Total Timesteps: 208843 Episode Num: 234 Reward: 880.7196440727187\n","Total Timesteps: 209843 Episode Num: 235 Reward: 663.1905995975466\n","Total Timesteps: 210843 Episode Num: 236 Reward: 590.3724071519694\n","---------------------------------------\n","Average Reward over the Evaluation Step: 661.387151\n","---------------------------------------\n","Total Timesteps: 211843 Episode Num: 237 Reward: 648.1402706292791\n","Total Timesteps: 212843 Episode Num: 238 Reward: 791.6212509122109\n","Total Timesteps: 213843 Episode Num: 239 Reward: 618.735577797801\n","Total Timesteps: 214843 Episode Num: 240 Reward: 714.9841245850148\n","Total Timesteps: 215843 Episode Num: 241 Reward: 518.8547667215689\n","---------------------------------------\n","Average Reward over the Evaluation Step: 759.070816\n","---------------------------------------\n","Total Timesteps: 216843 Episode Num: 242 Reward: 754.5579170608081\n","Total Timesteps: 217843 Episode Num: 243 Reward: 611.6581331256497\n","Total Timesteps: 218843 Episode Num: 244 Reward: 679.4399517245179\n","Total Timesteps: 219843 Episode Num: 245 Reward: 761.0631801017291\n","Total Timesteps: 220843 Episode Num: 246 Reward: 477.9461794442159\n","---------------------------------------\n","Average Reward over the Evaluation Step: 604.467870\n","---------------------------------------\n","Total Timesteps: 221843 Episode Num: 247 Reward: 439.6424147217194\n","Total Timesteps: 222843 Episode Num: 248 Reward: 454.596848318313\n","Total Timesteps: 223843 Episode Num: 249 Reward: 689.9245590416058\n","Total Timesteps: 224843 Episode Num: 250 Reward: 701.9287936035797\n","Total Timesteps: 225843 Episode Num: 251 Reward: 658.3257631517009\n","---------------------------------------\n","Average Reward over the Evaluation Step: 779.030066\n","---------------------------------------\n","Total Timesteps: 226843 Episode Num: 252 Reward: 485.17200106226807\n","Total Timesteps: 227843 Episode Num: 253 Reward: 890.733069709076\n","Total Timesteps: 228843 Episode Num: 254 Reward: 694.6930549387441\n","Total Timesteps: 229843 Episode Num: 255 Reward: 799.9086347957289\n","Total Timesteps: 230843 Episode Num: 256 Reward: 619.7889788568542\n","---------------------------------------\n","Average Reward over the Evaluation Step: 918.536642\n","---------------------------------------\n","Total Timesteps: 231843 Episode Num: 257 Reward: 941.0964619192177\n","Total Timesteps: 232843 Episode Num: 258 Reward: 848.1709207228479\n","Total Timesteps: 233843 Episode Num: 259 Reward: 941.4551873350101\n","Total Timesteps: 234843 Episode Num: 260 Reward: 961.2257554411547\n","Total Timesteps: 235843 Episode Num: 261 Reward: 747.8667835654867\n","---------------------------------------\n","Average Reward over the Evaluation Step: 696.173019\n","---------------------------------------\n","Total Timesteps: 236843 Episode Num: 262 Reward: 755.3836456724943\n","Total Timesteps: 237843 Episode Num: 263 Reward: 771.5944807854454\n","Total Timesteps: 238843 Episode Num: 264 Reward: 524.5202613482962\n","Total Timesteps: 239843 Episode Num: 265 Reward: 921.5971114998287\n","Total Timesteps: 240843 Episode Num: 266 Reward: 934.3031269751012\n","---------------------------------------\n","Average Reward over the Evaluation Step: 762.282701\n","---------------------------------------\n","Total Timesteps: 241843 Episode Num: 267 Reward: 811.8898058391358\n","Total Timesteps: 242843 Episode Num: 268 Reward: 859.594194499881\n","Total Timesteps: 243843 Episode Num: 269 Reward: 855.2051744087306\n","Total Timesteps: 244843 Episode Num: 270 Reward: 823.0265026331224\n","Total Timesteps: 245843 Episode Num: 271 Reward: 706.9505236067399\n","---------------------------------------\n","Average Reward over the Evaluation Step: 736.729248\n","---------------------------------------\n","Total Timesteps: 246843 Episode Num: 272 Reward: 755.4669148412712\n","Total Timesteps: 247843 Episode Num: 273 Reward: 856.7879257599529\n","Total Timesteps: 248843 Episode Num: 274 Reward: 765.1945498890137\n","Total Timesteps: 249843 Episode Num: 275 Reward: 592.6657043313165\n","Total Timesteps: 250843 Episode Num: 276 Reward: 765.1192847521161\n","---------------------------------------\n","Average Reward over the Evaluation Step: 862.114804\n","---------------------------------------\n","Total Timesteps: 251843 Episode Num: 277 Reward: 1036.212928064604\n","Total Timesteps: 252843 Episode Num: 278 Reward: 1016.999326842122\n","Total Timesteps: 253843 Episode Num: 279 Reward: 738.4023622954228\n","Total Timesteps: 254843 Episode Num: 280 Reward: 784.4808488304608\n","Total Timesteps: 255843 Episode Num: 281 Reward: 718.5842774855612\n","---------------------------------------\n","Average Reward over the Evaluation Step: 799.061089\n","---------------------------------------\n","Total Timesteps: 256843 Episode Num: 282 Reward: 827.4318019023849\n","Total Timesteps: 257843 Episode Num: 283 Reward: 751.0451759680924\n","Total Timesteps: 258843 Episode Num: 284 Reward: 694.9622339993444\n","Total Timesteps: 259843 Episode Num: 285 Reward: 832.5129113198999\n","Total Timesteps: 260843 Episode Num: 286 Reward: 907.6700617072535\n","---------------------------------------\n","Average Reward over the Evaluation Step: 686.759101\n","---------------------------------------\n","Total Timesteps: 261843 Episode Num: 287 Reward: 689.9077609691326\n","Total Timesteps: 262843 Episode Num: 288 Reward: 1061.1850911343874\n","Total Timesteps: 263843 Episode Num: 289 Reward: 824.2061482738211\n","Total Timesteps: 264843 Episode Num: 290 Reward: 638.4594175849659\n","Total Timesteps: 265843 Episode Num: 291 Reward: 1014.0726542036037\n","---------------------------------------\n","Average Reward over the Evaluation Step: 947.176086\n","---------------------------------------\n","Total Timesteps: 266843 Episode Num: 292 Reward: 638.6395433273632\n","Total Timesteps: 267843 Episode Num: 293 Reward: 1022.7303721852487\n","Total Timesteps: 268843 Episode Num: 294 Reward: 1143.6880255438941\n","Total Timesteps: 269843 Episode Num: 295 Reward: 950.4511722895348\n","Total Timesteps: 270843 Episode Num: 296 Reward: 726.1516359083508\n","---------------------------------------\n","Average Reward over the Evaluation Step: 925.861856\n","---------------------------------------\n","Total Timesteps: 271843 Episode Num: 297 Reward: 703.7127597042518\n","Total Timesteps: 272843 Episode Num: 298 Reward: 1059.784533426246\n","Total Timesteps: 273843 Episode Num: 299 Reward: 1326.5374793114797\n","Total Timesteps: 274843 Episode Num: 300 Reward: 976.6516494345323\n","Total Timesteps: 275843 Episode Num: 301 Reward: 903.2768267447227\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1009.909040\n","---------------------------------------\n","Total Timesteps: 276843 Episode Num: 302 Reward: 1090.5863325185503\n","Total Timesteps: 277843 Episode Num: 303 Reward: 1069.2133923451108\n","Total Timesteps: 278843 Episode Num: 304 Reward: 937.1641425109857\n","Total Timesteps: 279843 Episode Num: 305 Reward: 987.8413889818027\n","Total Timesteps: 280843 Episode Num: 306 Reward: 1223.4950387557128\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1077.144300\n","---------------------------------------\n","Total Timesteps: 281843 Episode Num: 307 Reward: 1151.6241856647473\n","Total Timesteps: 282843 Episode Num: 308 Reward: 1155.4405030599119\n","Total Timesteps: 283843 Episode Num: 309 Reward: 1081.6905363409899\n","Total Timesteps: 284843 Episode Num: 310 Reward: 1257.4219976586483\n","Total Timesteps: 285843 Episode Num: 311 Reward: 1203.3031288775946\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1159.739138\n","---------------------------------------\n","Total Timesteps: 286843 Episode Num: 312 Reward: 1161.8380584242345\n","Total Timesteps: 287843 Episode Num: 313 Reward: 1238.9559564290123\n","Total Timesteps: 288843 Episode Num: 314 Reward: 1222.7275670107722\n","Total Timesteps: 289843 Episode Num: 315 Reward: 1013.9738782639691\n","Total Timesteps: 290843 Episode Num: 316 Reward: 956.0253500032669\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1059.204678\n","---------------------------------------\n","Total Timesteps: 291843 Episode Num: 317 Reward: 662.478963039889\n","Total Timesteps: 292843 Episode Num: 318 Reward: 857.8503738412346\n","Total Timesteps: 293843 Episode Num: 319 Reward: 1183.2694572777223\n","Total Timesteps: 294843 Episode Num: 320 Reward: 888.6519174496154\n","Total Timesteps: 295843 Episode Num: 321 Reward: 1269.1288446670212\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1105.739941\n","---------------------------------------\n","Total Timesteps: 296843 Episode Num: 322 Reward: 710.1597267983069\n","Total Timesteps: 297843 Episode Num: 323 Reward: 952.2558896251954\n","Total Timesteps: 298843 Episode Num: 324 Reward: 1259.9644857854323\n","Total Timesteps: 299843 Episode Num: 325 Reward: 1021.9043458614375\n","Total Timesteps: 300843 Episode Num: 326 Reward: 1260.8707638687158\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1221.144939\n","---------------------------------------\n","Total Timesteps: 301843 Episode Num: 327 Reward: 1090.5815025038075\n","Total Timesteps: 302843 Episode Num: 328 Reward: 951.6861095376573\n","Total Timesteps: 303843 Episode Num: 329 Reward: 1349.6556155868916\n","Total Timesteps: 304843 Episode Num: 330 Reward: 1378.7204736640992\n","Total Timesteps: 305843 Episode Num: 331 Reward: 1321.5497107454369\n","---------------------------------------\n","Average Reward over the Evaluation Step: 953.624835\n","---------------------------------------\n","Total Timesteps: 306843 Episode Num: 332 Reward: 832.1679416333053\n","Total Timesteps: 307843 Episode Num: 333 Reward: 828.1989253580236\n","Total Timesteps: 308843 Episode Num: 334 Reward: 1224.9052874829754\n","Total Timesteps: 309843 Episode Num: 335 Reward: 836.7204475668751\n","Total Timesteps: 310843 Episode Num: 336 Reward: 1443.8968396121436\n","---------------------------------------\n","Average Reward over the Evaluation Step: 974.723523\n","---------------------------------------\n","Total Timesteps: 311843 Episode Num: 337 Reward: 815.4348782406785\n","Total Timesteps: 312843 Episode Num: 338 Reward: 1106.7604630102974\n","Total Timesteps: 313843 Episode Num: 339 Reward: 1293.6626891571063\n","Total Timesteps: 314843 Episode Num: 340 Reward: 1453.1237565713939\n","Total Timesteps: 315843 Episode Num: 341 Reward: 1338.238286074257\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1394.774211\n","---------------------------------------\n","Total Timesteps: 316843 Episode Num: 342 Reward: 1288.840437913289\n","Total Timesteps: 317843 Episode Num: 343 Reward: 1314.6875773542433\n","Total Timesteps: 318843 Episode Num: 344 Reward: 1379.9283957592147\n","Total Timesteps: 319843 Episode Num: 345 Reward: 1278.7901840184143\n","Total Timesteps: 320843 Episode Num: 346 Reward: 1534.7775537335997\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1051.763459\n","---------------------------------------\n","Total Timesteps: 321843 Episode Num: 347 Reward: 936.5283171293192\n","Total Timesteps: 322843 Episode Num: 348 Reward: 1476.3280894262125\n","Total Timesteps: 323843 Episode Num: 349 Reward: 1354.1592944905863\n","Total Timesteps: 324843 Episode Num: 350 Reward: 1599.249715136585\n","Total Timesteps: 325843 Episode Num: 351 Reward: 1153.4766429267409\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1616.410025\n","---------------------------------------\n","Total Timesteps: 326843 Episode Num: 352 Reward: 1695.389040419787\n","Total Timesteps: 327843 Episode Num: 353 Reward: 945.7057959964726\n","Total Timesteps: 328843 Episode Num: 354 Reward: 1535.1430297422175\n","Total Timesteps: 329843 Episode Num: 355 Reward: 1659.0094838455827\n","Total Timesteps: 330843 Episode Num: 356 Reward: 1575.7364981816775\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1189.607465\n","---------------------------------------\n","Total Timesteps: 331843 Episode Num: 357 Reward: 1016.5765097145319\n","Total Timesteps: 332843 Episode Num: 358 Reward: 1338.1834525675356\n","Total Timesteps: 333843 Episode Num: 359 Reward: 1493.3400269776344\n","Total Timesteps: 334843 Episode Num: 360 Reward: 1743.1477971006614\n","Total Timesteps: 335843 Episode Num: 361 Reward: 1654.258886800809\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1536.641259\n","---------------------------------------\n","Total Timesteps: 336843 Episode Num: 362 Reward: 1210.1357868861674\n","Total Timesteps: 337843 Episode Num: 363 Reward: 1683.3736444311191\n","Total Timesteps: 338843 Episode Num: 364 Reward: 1388.2352527779647\n","Total Timesteps: 339843 Episode Num: 365 Reward: 1335.2955207372995\n","Total Timesteps: 340843 Episode Num: 366 Reward: 1428.673262656668\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1721.293623\n","---------------------------------------\n","Total Timesteps: 341843 Episode Num: 367 Reward: 1691.760867896142\n","Total Timesteps: 342843 Episode Num: 368 Reward: 1667.0267824278799\n","Total Timesteps: 343843 Episode Num: 369 Reward: 1566.0986211069064\n","Total Timesteps: 344843 Episode Num: 370 Reward: 1607.5950627937007\n","Total Timesteps: 345843 Episode Num: 371 Reward: 1568.9898483446107\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1627.916060\n","---------------------------------------\n","Total Timesteps: 346843 Episode Num: 372 Reward: 1642.0501857303532\n","Total Timesteps: 347843 Episode Num: 373 Reward: 1678.7571551030176\n","Total Timesteps: 348843 Episode Num: 374 Reward: 1700.1932427277206\n","Total Timesteps: 349843 Episode Num: 375 Reward: 1593.339521112722\n","Total Timesteps: 350843 Episode Num: 376 Reward: 1649.964561995703\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1798.847166\n","---------------------------------------\n","Total Timesteps: 351843 Episode Num: 377 Reward: 1793.5942344890223\n","Total Timesteps: 352843 Episode Num: 378 Reward: 1671.5063723860046\n","Total Timesteps: 353843 Episode Num: 379 Reward: 1618.8632141726869\n","Total Timesteps: 354843 Episode Num: 380 Reward: 1809.8593421987557\n","Total Timesteps: 355843 Episode Num: 381 Reward: 1748.4271628118274\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2056.312892\n","---------------------------------------\n","Total Timesteps: 356843 Episode Num: 382 Reward: 2073.445879391569\n","Total Timesteps: 357843 Episode Num: 383 Reward: 1765.7113135075056\n","Total Timesteps: 358843 Episode Num: 384 Reward: 1916.6509771024898\n","Total Timesteps: 359843 Episode Num: 385 Reward: 1715.6906758981795\n","Total Timesteps: 360843 Episode Num: 386 Reward: 1909.1398225759922\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1883.069787\n","---------------------------------------\n","Total Timesteps: 361843 Episode Num: 387 Reward: 1831.8258793190807\n","Total Timesteps: 362843 Episode Num: 388 Reward: 1741.9569369909705\n","Total Timesteps: 363843 Episode Num: 389 Reward: 1733.9190219987777\n","Total Timesteps: 364843 Episode Num: 390 Reward: 789.1305959197787\n","Total Timesteps: 365843 Episode Num: 391 Reward: 1925.2722393418571\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2079.259657\n","---------------------------------------\n","Total Timesteps: 366843 Episode Num: 392 Reward: 2065.5455400039914\n","Total Timesteps: 367843 Episode Num: 393 Reward: 1784.2925897584419\n","Total Timesteps: 368843 Episode Num: 394 Reward: 1734.5339784872758\n","Total Timesteps: 369843 Episode Num: 395 Reward: 1999.8377529887348\n","Total Timesteps: 370843 Episode Num: 396 Reward: 1839.7884556172917\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2069.115192\n","---------------------------------------\n","Total Timesteps: 371843 Episode Num: 397 Reward: 2061.2074790505512\n","Total Timesteps: 372843 Episode Num: 398 Reward: 1991.4320049324315\n","Total Timesteps: 373843 Episode Num: 399 Reward: 1911.0645604518936\n","Total Timesteps: 374843 Episode Num: 400 Reward: 1868.8271298139814\n","Total Timesteps: 375843 Episode Num: 401 Reward: 1861.4455801934246\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2058.884762\n","---------------------------------------\n","Total Timesteps: 376843 Episode Num: 402 Reward: 1996.49881896218\n","Total Timesteps: 377843 Episode Num: 403 Reward: 2084.0331425160284\n","Total Timesteps: 378843 Episode Num: 404 Reward: 1967.5742977549803\n","Total Timesteps: 379843 Episode Num: 405 Reward: 2017.022058805357\n","Total Timesteps: 380843 Episode Num: 406 Reward: 2076.6999461173737\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2108.023802\n","---------------------------------------\n","Total Timesteps: 381843 Episode Num: 407 Reward: 2101.135659210777\n","Total Timesteps: 382843 Episode Num: 408 Reward: 2048.3600037654132\n","Total Timesteps: 383843 Episode Num: 409 Reward: 1974.1724009935042\n","Total Timesteps: 384843 Episode Num: 410 Reward: 2135.385996241932\n","Total Timesteps: 385843 Episode Num: 411 Reward: 2017.0152103676678\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2001.663784\n","---------------------------------------\n","Total Timesteps: 386843 Episode Num: 412 Reward: 1989.8834688355491\n","Total Timesteps: 387843 Episode Num: 413 Reward: 1956.2162514959682\n","Total Timesteps: 388843 Episode Num: 414 Reward: 2083.83305742138\n","Total Timesteps: 389843 Episode Num: 415 Reward: 2001.8877227302498\n","Total Timesteps: 390843 Episode Num: 416 Reward: 1929.6169843398977\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2196.604938\n","---------------------------------------\n","Total Timesteps: 391843 Episode Num: 417 Reward: 2188.9955345796498\n","Total Timesteps: 392843 Episode Num: 418 Reward: 2036.8144655381568\n","Total Timesteps: 393843 Episode Num: 419 Reward: 2103.1528350191816\n","Total Timesteps: 394843 Episode Num: 420 Reward: 1998.831517089925\n","Total Timesteps: 395843 Episode Num: 421 Reward: 1907.9366077814525\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2236.232816\n","---------------------------------------\n","Total Timesteps: 396843 Episode Num: 422 Reward: 2166.4321968679237\n","Total Timesteps: 397843 Episode Num: 423 Reward: 1909.3113533776152\n","Total Timesteps: 398843 Episode Num: 424 Reward: 2023.9112566182828\n","Total Timesteps: 399843 Episode Num: 425 Reward: 2162.4136802224302\n","Total Timesteps: 400843 Episode Num: 426 Reward: 2170.309281207171\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2079.531775\n","---------------------------------------\n","Total Timesteps: 401843 Episode Num: 427 Reward: 2047.8910370433816\n","Total Timesteps: 402843 Episode Num: 428 Reward: 2135.53075555811\n","Total Timesteps: 403843 Episode Num: 429 Reward: 2319.0949111213445\n","Total Timesteps: 404843 Episode Num: 430 Reward: 2135.189836552471\n","Total Timesteps: 405843 Episode Num: 431 Reward: 2131.1820277624793\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2204.637879\n","---------------------------------------\n","Total Timesteps: 406843 Episode Num: 432 Reward: 2103.274869528777\n","Total Timesteps: 407843 Episode Num: 433 Reward: 2295.17106675173\n","Total Timesteps: 408843 Episode Num: 434 Reward: 2137.295763270613\n","Total Timesteps: 409843 Episode Num: 435 Reward: 2223.7003183051315\n","Total Timesteps: 410843 Episode Num: 436 Reward: 2147.1768905283866\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2192.940977\n","---------------------------------------\n","Total Timesteps: 411843 Episode Num: 437 Reward: 2196.2617808297796\n","Total Timesteps: 412843 Episode Num: 438 Reward: 2169.385917010183\n","Total Timesteps: 413843 Episode Num: 439 Reward: 2065.3568185515724\n","Total Timesteps: 414843 Episode Num: 440 Reward: 2094.7146077847588\n","Total Timesteps: 415843 Episode Num: 441 Reward: 2174.728722492527\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2230.704992\n","---------------------------------------\n","Total Timesteps: 416843 Episode Num: 442 Reward: 2197.1065527221326\n","Total Timesteps: 417843 Episode Num: 443 Reward: 2243.788814869117\n","Total Timesteps: 418843 Episode Num: 444 Reward: 2353.0465385508096\n","Total Timesteps: 419843 Episode Num: 445 Reward: 2272.7167270122704\n","Total Timesteps: 420843 Episode Num: 446 Reward: 2217.3344897095217\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2365.281012\n","---------------------------------------\n","Total Timesteps: 421843 Episode Num: 447 Reward: 2373.725842423853\n","Total Timesteps: 422843 Episode Num: 448 Reward: 2282.2861862541295\n","Total Timesteps: 423843 Episode Num: 449 Reward: 2285.008201377846\n","Total Timesteps: 424843 Episode Num: 450 Reward: 2279.3323687477205\n","Total Timesteps: 425843 Episode Num: 451 Reward: 2076.5627677483612\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2053.933000\n","---------------------------------------\n","Total Timesteps: 426843 Episode Num: 452 Reward: 2024.4031680195433\n","Total Timesteps: 427843 Episode Num: 453 Reward: 2106.6838509751046\n","Total Timesteps: 428843 Episode Num: 454 Reward: 2217.4149176029036\n","Total Timesteps: 429843 Episode Num: 455 Reward: 2187.0917479236605\n","Total Timesteps: 430843 Episode Num: 456 Reward: 2381.211183812339\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2372.887772\n","---------------------------------------\n","Total Timesteps: 431843 Episode Num: 457 Reward: 2372.1637296388158\n","Total Timesteps: 432843 Episode Num: 458 Reward: 2406.542562973155\n","Total Timesteps: 433843 Episode Num: 459 Reward: 2368.3387632503095\n","Total Timesteps: 434843 Episode Num: 460 Reward: 2205.66631066422\n","Total Timesteps: 435843 Episode Num: 461 Reward: 2034.274790906779\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2336.762666\n","---------------------------------------\n","Total Timesteps: 436843 Episode Num: 462 Reward: 2286.2350904940627\n","Total Timesteps: 437843 Episode Num: 463 Reward: 2332.7876119873713\n","Total Timesteps: 438843 Episode Num: 464 Reward: 2260.647737676505\n","Total Timesteps: 439843 Episode Num: 465 Reward: 2334.546272578662\n","Total Timesteps: 440843 Episode Num: 466 Reward: 2213.0024880927576\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2317.219465\n","---------------------------------------\n","Total Timesteps: 441843 Episode Num: 467 Reward: 2278.8277181238\n","Total Timesteps: 442843 Episode Num: 468 Reward: 2356.81781291284\n","Total Timesteps: 443843 Episode Num: 469 Reward: 2314.4525876909106\n","Total Timesteps: 444843 Episode Num: 470 Reward: 2260.810396225663\n","Total Timesteps: 445843 Episode Num: 471 Reward: 2414.5318046423013\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2430.657401\n","---------------------------------------\n","Total Timesteps: 446843 Episode Num: 472 Reward: 2447.5244033582594\n","Total Timesteps: 447843 Episode Num: 473 Reward: 2337.644773609951\n","Total Timesteps: 448843 Episode Num: 474 Reward: 2228.3234341908355\n","Total Timesteps: 449843 Episode Num: 475 Reward: 2342.6381133879604\n","Total Timesteps: 450843 Episode Num: 476 Reward: 2286.822050688777\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2316.385047\n","---------------------------------------\n","Total Timesteps: 451843 Episode Num: 477 Reward: 2250.752712368103\n","Total Timesteps: 452843 Episode Num: 478 Reward: 2358.266460449245\n","Total Timesteps: 453843 Episode Num: 479 Reward: 2304.9336614330464\n","Total Timesteps: 454843 Episode Num: 480 Reward: 2209.357730527879\n","Total Timesteps: 455843 Episode Num: 481 Reward: 2297.905142030388\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2304.791951\n","---------------------------------------\n","Total Timesteps: 456843 Episode Num: 482 Reward: 2252.8072829058246\n","Total Timesteps: 457843 Episode Num: 483 Reward: 2382.7545843506014\n","Total Timesteps: 458843 Episode Num: 484 Reward: 2335.330246056108\n","Total Timesteps: 459843 Episode Num: 485 Reward: 2380.3089930992937\n","Total Timesteps: 460843 Episode Num: 486 Reward: 2299.868254998144\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2404.757265\n","---------------------------------------\n","Total Timesteps: 461843 Episode Num: 487 Reward: 2323.2594753936723\n","Total Timesteps: 462843 Episode Num: 488 Reward: 2212.933422808128\n","Total Timesteps: 463843 Episode Num: 489 Reward: 2189.157606387318\n","Total Timesteps: 464843 Episode Num: 490 Reward: 2261.744525968842\n","Total Timesteps: 465843 Episode Num: 491 Reward: 2364.9424414540717\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2398.612792\n","---------------------------------------\n","Total Timesteps: 466843 Episode Num: 492 Reward: 2390.368928189811\n","Total Timesteps: 467843 Episode Num: 493 Reward: 2367.1240901839947\n","Total Timesteps: 468843 Episode Num: 494 Reward: 2244.0458952097424\n","Total Timesteps: 469843 Episode Num: 495 Reward: 2170.253785305471\n","Total Timesteps: 470843 Episode Num: 496 Reward: 2345.888678009939\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2338.601629\n","---------------------------------------\n","Total Timesteps: 471843 Episode Num: 497 Reward: 2303.5267764179594\n","Total Timesteps: 472843 Episode Num: 498 Reward: 2342.5548063603496\n","Total Timesteps: 473843 Episode Num: 499 Reward: 2417.0267860006975\n","Total Timesteps: 474843 Episode Num: 500 Reward: 2333.9269761063447\n","Total Timesteps: 475843 Episode Num: 501 Reward: 2197.669025922341\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2269.340777\n","---------------------------------------\n","Total Timesteps: 476843 Episode Num: 502 Reward: 2231.3441719335383\n","Total Timesteps: 477843 Episode Num: 503 Reward: 2351.161288015393\n","Total Timesteps: 478843 Episode Num: 504 Reward: 2362.782051065752\n","Total Timesteps: 479843 Episode Num: 505 Reward: 2331.2837526889143\n","Total Timesteps: 480843 Episode Num: 506 Reward: 2387.9776594675595\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2476.728210\n","---------------------------------------\n","Total Timesteps: 481843 Episode Num: 507 Reward: 2445.9542716941587\n","Total Timesteps: 482843 Episode Num: 508 Reward: 2343.7621364706406\n","Total Timesteps: 483843 Episode Num: 509 Reward: 2345.752654683093\n","Total Timesteps: 484843 Episode Num: 510 Reward: 2359.870977279916\n","Total Timesteps: 485843 Episode Num: 511 Reward: 2442.6296588071677\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2388.602904\n","---------------------------------------\n","Total Timesteps: 486843 Episode Num: 512 Reward: 2348.1414298984987\n","Total Timesteps: 487843 Episode Num: 513 Reward: 2339.6855148413956\n","Total Timesteps: 488843 Episode Num: 514 Reward: 2345.055246664464\n","Total Timesteps: 489843 Episode Num: 515 Reward: 2333.2528375112333\n","Total Timesteps: 490843 Episode Num: 516 Reward: 2333.025209486013\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2401.007767\n","---------------------------------------\n","Total Timesteps: 491843 Episode Num: 517 Reward: 2352.4587812711916\n","Total Timesteps: 492843 Episode Num: 518 Reward: 2363.3499273885636\n","Total Timesteps: 493843 Episode Num: 519 Reward: 2392.0274465047733\n","Total Timesteps: 494843 Episode Num: 520 Reward: 2301.1890070736376\n","Total Timesteps: 495843 Episode Num: 521 Reward: 2326.086985290174\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2462.894869\n","---------------------------------------\n","Total Timesteps: 496843 Episode Num: 522 Reward: 2394.2353193582608\n","Total Timesteps: 497843 Episode Num: 523 Reward: 2271.458279716304\n","Total Timesteps: 498843 Episode Num: 524 Reward: 2415.385861386886\n","Total Timesteps: 499843 Episode Num: 525 Reward: 2450.298765407715\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2442.126929\n","---------------------------------------\n"]}]},{"cell_type":"markdown","source":["## Inference"],"metadata":{"id":"5FEmvzRfWvc_"}},{"cell_type":"code","source":["!pip install gym==0.15.3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lBZ1z_dEClGW","executionInfo":{"status":"ok","timestamp":1701982211831,"user_tz":-330,"elapsed":13075,"user":{"displayName":"Hema","userId":"12688812517772780722"}},"outputId":"6030dcfa-c4da-4e76-d375-b38050fe097f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gym==0.15.3 in /usr/local/lib/python3.10/dist-packages (0.15.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym==0.15.3) (1.11.4)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.15.3) (1.23.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gym==0.15.3) (1.16.0)\n","Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.15.3) (1.3.2)\n","Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.15.3) (1.2.2)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym==0.15.3) (0.18.3)\n"]}]},{"cell_type":"code","source":["from gym import wrappers"],"metadata":{"id":"M4gOxgK_CnyA"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701982476156,"user_tz":-330,"elapsed":87762,"user":{"displayName":"Hema","userId":"12688812517772780722"}},"outputId":"b7078d73-8db8-450e-d56b-228fa7e56d6b","id":"pqiWtGonWpuB"},"source":["class Actor(nn.Module):\n","\n","  def __init__(self, state_dim, action_dim, max_action):\n","    super(Actor, self).__init__()\n","    self.layer_1 = nn.Linear(state_dim, 400)\n","    self.layer_2 = nn.Linear(400, 300)\n","    self.layer_3 = nn.Linear(300, action_dim)\n","    self.max_action = max_action\n","\n","  def forward(self, x):\n","    x = F.relu(self.layer_1(x))\n","    x = F.relu(self.layer_2(x))\n","    x = self.max_action * torch.tanh(self.layer_3(x))\n","    return x\n","\n","class Critic(nn.Module):\n","\n","  def __init__(self, state_dim, action_dim):\n","    super(Critic, self).__init__()\n","    # Defining the first Critic neural network\n","    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n","    self.layer_2 = nn.Linear(400, 300)\n","    self.layer_3 = nn.Linear(300, 1)\n","    # Defining the second Critic neural network\n","    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n","    self.layer_5 = nn.Linear(400, 300)\n","    self.layer_6 = nn.Linear(300, 1)\n","\n","  def forward(self, x, u):\n","    xu = torch.cat([x, u], 1)\n","    # Forward-Propagation on the first Critic Neural Network\n","    x1 = F.relu(self.layer_1(xu))\n","    x1 = F.relu(self.layer_2(x1))\n","    x1 = self.layer_3(x1)\n","    # Forward-Propagation on the second Critic Neural Network\n","    x2 = F.relu(self.layer_4(xu))\n","    x2 = F.relu(self.layer_5(x2))\n","    x2 = self.layer_6(x2)\n","    return x1, x2\n","\n","  def Q1(self, x, u):\n","    xu = torch.cat([x, u], 1)\n","    x1 = F.relu(self.layer_1(xu))\n","    x1 = F.relu(self.layer_2(x1))\n","    x1 = self.layer_3(x1)\n","    return x1\n","\n","# Selecting the device (CPU or GPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Building the whole Training Process into a class\n","\n","class TD3(object):\n","\n","  def __init__(self, state_dim, action_dim, max_action):\n","    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n","    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n","    self.actor_target.load_state_dict(self.actor.state_dict())\n","    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n","    self.critic = Critic(state_dim, action_dim).to(device)\n","    self.critic_target = Critic(state_dim, action_dim).to(device)\n","    self.critic_target.load_state_dict(self.critic.state_dict())\n","    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n","    self.max_action = max_action\n","\n","  def select_action(self, state):\n","    state = torch.Tensor(state.reshape(1, -1)).to(device)\n","    return self.actor(state).cpu().data.numpy().flatten()\n","\n","  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n","\n","    for it in range(iterations):\n","\n","      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n","      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n","      state = torch.Tensor(batch_states).to(device)\n","      next_state = torch.Tensor(batch_next_states).to(device)\n","      action = torch.Tensor(batch_actions).to(device)\n","      reward = torch.Tensor(batch_rewards).to(device)\n","      done = torch.Tensor(batch_dones).to(device)\n","\n","      # Step 5: From the next state s’, the Actor target plays the next action a’\n","      next_action = self.actor_target(next_state)\n","\n","      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n","      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n","      noise = noise.clamp(-noise_clip, noise_clip)\n","      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n","\n","      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n","      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n","\n","      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n","      target_Q = torch.min(target_Q1, target_Q2)\n","\n","      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n","      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n","\n","      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n","      current_Q1, current_Q2 = self.critic(state, action)\n","\n","      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n","      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n","\n","      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n","      self.critic_optimizer.zero_grad()\n","      critic_loss.backward()\n","      self.critic_optimizer.step()\n","\n","      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n","      if it % policy_freq == 0:\n","        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n","        self.actor_optimizer.zero_grad()\n","        actor_loss.backward()\n","        self.actor_optimizer.step()\n","\n","        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n","        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n","          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n","\n","        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n","        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n","          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n","\n","  # Making a save method to save a trained model\n","  def save(self, filename, directory):\n","    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n","    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n","\n","  # Making a load method to load a pre-trained model\n","  def load(self, filename, directory):\n","    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n","    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n","\n","def evaluate_policy(policy, eval_episodes=10):\n","  avg_reward = 0.\n","  for _ in range(eval_episodes):\n","    obs = env.reset()\n","    done = False\n","    while not done:\n","      action = policy.select_action(np.array(obs))\n","      obs, reward, done, _ = env.step(action)\n","      avg_reward += reward\n","  avg_reward /= eval_episodes\n","  print (\"---------------------------------------\")\n","  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n","  print (\"---------------------------------------\")\n","  return avg_reward\n","\n","env_name = \"AntBulletEnv-v0\"\n","seed = 0\n","\n","file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n","print (\"---------------------------------------\")\n","print (\"Settings: %s\" % (file_name))\n","print (\"---------------------------------------\")\n","\n","eval_episodes = 10\n","save_env_vid = True\n","env = gym.make(env_name)\n","max_episode_steps = env._max_episode_steps\n","if save_env_vid:\n","  env = wrappers.Monitor(env, monitor_dir, force = True)\n","  env.reset()\n","env.seed(seed)\n","torch.manual_seed(seed)\n","np.random.seed(seed)\n","state_dim = env.observation_space.shape[0]\n","action_dim = env.action_space.shape[0]\n","max_action = float(env.action_space.high[0])\n","policy = TD3(state_dim, action_dim, max_action)\n","policy.load(file_name, './pytorch_models/')\n","_ = evaluate_policy(policy, eval_episodes=eval_episodes)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["---------------------------------------\n","Settings: TD3_AntBulletEnv-v0_0\n","---------------------------------------\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2014.708528\n","---------------------------------------\n"]}]}]}